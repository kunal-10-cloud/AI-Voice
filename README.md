Voice Agent ‚Äì Engineering Assignment 1

A production-ready, low-latency, real-time voice assistant capable of natural, interruptible conversations with multiple concurrent users. The system is built with a strong focus on latency, robustness, observability, and engineering judgment, closely mirroring real-world voice AI systems.

‚∏ª

 Project Overview

This project implements a cascaded, streaming voice pipeline that allows users to talk to an AI agent as naturally as they would to a human. The agent supports:
	‚Ä¢	Real-time speech recognition
	‚Ä¢	Natural turn-taking and barge-in (interruptions)
	‚Ä¢	Dynamic context updates during an active conversation
	‚Ä¢	Real-time web search for current information
	‚Ä¢	Session-scoped conversation memory
	‚Ä¢	Multi-user concurrent sessions
	‚Ä¢	End-to-end observability with detailed metrics
	‚Ä¢	Provider fallback for resilience

The system is designed to degrade gracefully, remain responsive under load, and expose internal performance characteristics clearly.

‚∏ª

 Voice Pipeline Architecture

User Audio
   ‚Üì
Noise Handling (lightweight)
   ‚Üì
Custom VAD (Voice Activity Detection)
   ‚Üì
Turn Detection (state-machine + grace period)
   ‚Üì
Streaming STT (Deepgram)
   ‚Üì
LLM Reasoning + Tools (Groq + Web Search)
   ‚Üì
Streaming TTS (Deepgram)
   ‚Üì
Audio Playback (with barge-in support)

Each stage is isolated, observable, and instrumented for latency and correctness.

‚∏ª
![alt text](image.png)

Architecture Overview

Backend (Node.js)
	‚Ä¢	WebSocket-based server for low-latency, bidirectional audio and control messages
	‚Ä¢	Session Manager to isolate users and prevent context bleed
	‚Ä¢	State-machine driven turn detection (speech_start ‚Üí speech_end)
	‚Ä¢	Streaming integrations for STT and TTS
	‚Ä¢	LLM orchestration layer with tool invocation and fallback logic

Frontend (React)
	‚Ä¢	Real-time microphone capture
	‚Ä¢	Visual agent avatar with speaking/listening/thinking states
	‚Ä¢	Live transcripts for both user and assistant
	‚Ä¢	Left-hand metrics sidebar for per-turn observability

‚∏ª

üîä Custom Audio Processing

Voice Activity Detection (VAD)
	‚Ä¢	Implemented using energy-based heuristics on raw PCM audio
	‚Ä¢	Converts Int16 PCM ‚Üí Float32 for accurate signal processing
	‚Ä¢	Tuned thresholds to balance responsiveness and false positives

Turn Detection
	‚Ä¢	Driven by a VAD-gated state machine
	‚Ä¢	Includes a grace period to avoid cutting off trailing phonemes
	‚Ä¢	Prevents transcript fragmentation and improves STT accuracy

‚∏ª

üó£Ô∏è Speech-to-Text (STT)
	‚Ä¢	Primary Provider: Deepgram (Streaming WebSocket)
	‚Ä¢	Always-on STT connection to eliminate startup latency
	‚Ä¢	Interim transcripts are replaced, final transcripts are appended
	‚Ä¢	Downstream logic runs only on final transcripts

STT Stability Improvements
	‚Ä¢	Turn-final buffering (no partial-word commits)
	‚Ä¢	Confidence-based validation
	‚Ä¢	Clarification prompts on ambiguous or low-confidence transcripts

STT Fallback
	‚Ä¢	Fallback Provider: AssemblyAI (HTTP, final transcript only)
	‚Ä¢	Triggered on connection failure, timeout, or provider errors

‚∏ª

LLM Processing
	‚Ä¢	Primary Provider: Groq (fast inference)
	‚Ä¢	Secondary Provider: Groq (separate API key for fallback simulation)

Features
	‚Ä¢	Session-scoped conversation memory (bounded sliding window)
	‚Ä¢	Tool-based reasoning for web search
	‚Ä¢	Prompting optimized for spoken, human-like responses (not bullet points)
	‚Ä¢	Numerical and temperature responses formatted for natural speech

LLM Fallback
	‚Ä¢	Automatic fallback on rate limits, timeouts, or network failures
	‚Ä¢	Transparent to the user
	‚Ä¢	Logged and surfaced in metrics

‚∏ª

 Web Search Integration
	‚Ä¢	Provider: Tavily Search API
	‚Ä¢	Intent-gated search (only triggers for time-sensitive / external queries)
	‚Ä¢	Search results are:
	‚Ä¢	Injected ephemerally into the prompt
	‚Ä¢	Never stored in long-term conversation memory
	‚Ä¢	Sources are cited in responses

‚∏ª

Text-to-Speech (TTS)
	‚Ä¢	Primary Provider: Deepgram TTS (streaming)
	‚Ä¢	Audio streamed in browser-compatible format
	‚Ä¢	Frontend uses a dedicated playback AudioContext

Barge-In Support
	‚Ä¢	User can interrupt the assistant mid-speech
	‚Ä¢	TTS stream is immediately cancelled
	‚Ä¢	Playback buffer cleared
	‚Ä¢	New user speech is captured without delay

TTS Fallback
	‚Ä¢	Secondary Provider: Cartesia (HTTP synthesis)
	‚Ä¢	Triggered if streaming TTS fails to start or stalls

‚∏ª

 Real-Time Context Updates
	‚Ä¢	Context can be injected into an active session via WebSocket
	‚Ä¢	Used for:
	‚Ä¢	Changing assistant persona
	‚Ä¢	Applying admin instructions
	‚Ä¢	Context updates:
	‚Ä¢	Are session-scoped
	‚Ä¢	Do not pollute conversation memory
	‚Ä¢	Apply immediately to subsequent responses

‚∏ª

 Multi-User Architecture
	‚Ä¢	Each WebSocket connection maps to an isolated session
	‚Ä¢	No shared mutable state between sessions
	‚Ä¢	Designed to scale horizontally

Scalability Notes
	‚Ä¢	10‚Äì100 concurrent users: Single Node.js instance
	‚Ä¢	1000+ users: Horizontal scaling with:
	‚Ä¢	Stateless WebSocket gateways
	‚Ä¢	External session store (e.g., Redis)
	‚Ä¢	Provider-side scaling for STT/LLM/TTS

‚∏ª

 Observability & Metrics Dashboard

For every conversation turn, the system records:
	‚Ä¢	VAD detection timestamps
	‚Ä¢	STT latency
	‚Ä¢	LLM latency (including TTFT)
	‚Ä¢	TTS latency
	‚Ä¢	End-to-end turn latency
	‚Ä¢	Search triggered / skipped
	‚Ä¢	Provider fallback indicators

Metrics are displayed live in the UI sidebar and logged structurally.

‚∏ª

 Structured Logging
	‚Ä¢	JSON logs with correlation IDs
	‚Ä¢	Each log includes:
	‚Ä¢	sessionId
	‚Ä¢	turnId
	‚Ä¢	pipeline stage
	‚Ä¢	timestamp

This allows tracing a single user utterance end-to-end across the system.

‚∏ª

 Conversation Memory
	‚Ä¢	Session-scoped, bounded memory
	‚Ä¢	Last N messages sent to the LLM
	‚Ä¢	Prevents token explosion while preserving context

Future Extensions
	‚Ä¢	Redis-backed persistence
	‚Ä¢	Long-term summarization

‚∏ª

 Testing & Verification
	‚Ä¢	Manual testing with multiple concurrent browser tabs
	‚Ä¢	Admin scripts for:
	‚Ä¢	Live context injection
	‚Ä¢	Session targeting
	‚Ä¢	Simulated provider failures to verify fallback logic

‚∏ª

‚öñÔ∏è Tradeoffs, Iterations & Design Decisions

This project went through multiple iterations while solving real, production-style problems. Below is a transparent account of what didn‚Äôt work initially, why changes were made, and the final tradeoffs.

1. STT Stability vs Latency

Initial approach:
	‚Ä¢	Triggered STT transcription aggressively on short silences
	‚Ä¢	Processed partial transcripts immediately

Problems encountered:
	‚Ä¢	Broken words (e.g., ‚ÄúPun‚Äù instead of ‚ÄúPune‚Äù)
	‚Ä¢	Truncated entities and phonemes
	‚Ä¢	Over-reliance on transcript normalization hacks

Final decision:
	‚Ä¢	Move to turn-final transcription only
	‚Ä¢	Introduce a grace period after silence detection
	‚Ä¢	Always-on STT socket to avoid startup delays

Tradeoff:
	‚Ä¢	Slightly higher end-of-turn latency
	‚Ä¢	Significantly higher transcript accuracy and naturalness

‚∏ª

2. WAV / MP3 Streaming vs Raw PCM Playback

Initial approach:
	‚Ä¢	Stream raw PCM (Linear16) audio
	‚Ä¢	Manually convert Int16 ‚Üí Float32 ‚Üí AudioBuffer in frontend

Problems encountered:
	‚Ä¢	Skipped words
	‚Ä¢	Partial audio playback
	‚Ä¢	Decode failures due to missing headers

Final decision:
	‚Ä¢	Streaming WAV from TTS provider
	‚Ä¢	Decoding each chunk individually in the browser

Tradeoff:
	‚Ä¢	More frontend audio logic
	‚Ä¢	Deterministic, glitch-free playback with no skipped speech

‚∏ª

3. Tool-Based LLM Search vs Prompt-Based Intent Gating

Initial approach:
	‚Ä¢	Native LLM tool-calling for web search

Problems encountered:
	‚Ä¢	Tool call failures
	‚Ä¢	Over-triggering on generic queries
	‚Ä¢	Poor debuggability

Final decision:
	‚Ä¢	Explicit intent-gating logic
	‚Ä¢	Two-step LLM flow: decision ‚Üí grounded response

Tradeoff:
	‚Ä¢	Slightly more orchestration code
	‚Ä¢	Full control, predictability, and clean logs

‚∏ª

4. Barge-In Complexity vs System Stability

Initial approach:
	‚Ä¢	Attempted to handle barge-in and audio correctness together

Problems encountered:
	‚Ä¢	Race conditions
	‚Ä¢	Inconsistent TTS cancellation
	‚Ä¢	Unreliable UX

Final decision:
	‚Ä¢	First stabilize audio correctness
	‚Ä¢	Re-introduce barge-in with:
	‚Ä¢	request IDs
	‚Ä¢	atomic TTS cancellation
	‚Ä¢	buffer clearing

Tradeoff:
	‚Ä¢	Longer implementation time
	‚Ä¢	Clean, reliable interruption behavior

‚∏ª

5. Provider Choice & Fallback Strategy

Initial approach:
	‚Ä¢	Single provider per capability

Problems encountered:
	‚Ä¢	Rate limits during barge-in
	‚Ä¢	Hard failures blocking the pipeline

Final decision:
	‚Ä¢	Explicit fallback layers for STT, LLM, and TTS
	‚Ä¢	Secondary Groq key used to simulate provider isolation

Tradeoff:
	‚Ä¢	Slightly more configuration
	‚Ä¢	High resilience and production realism

‚∏ª

6. What Was Deferred Intentionally
	‚Ä¢	Heavy DSP-based noise suppression
	‚Ä¢	Persistent long-term memory
	‚Ä¢	Authentication & access control

These were deferred to keep the focus on core voice interaction quality, latency, and robustness.

This iterative process reflects real-world engineering tradeoffs rather than idealized designs.

‚∏ª

 Future Work
	‚Ä¢	Advanced noise suppression
	‚Ä¢	Semantic caching for repeated queries
	‚Ä¢	Full deployment with autoscaling
	‚Ä¢	

‚∏ª

üõ†Ô∏è Setup Instructions

Prerequisites
	‚Ä¢	Node.js (v18+ recommended)
	‚Ä¢	Modern browser (Chrome preferred)

Installation

git clone <repo-url>
cd backend
npm install

Environment Variables

Create a .env file based on .env.example:

DEEPGRAM_API_KEY=
GROQ_API_KEY_PRIMARY=
GROQ_API_KEY_SECONDARY=
TAVILY_API_KEY=

Run Locally

node server.js

Open the frontend and start a voice session.

‚∏ª

 Demo

A 3‚Äì4 minute demo video showcasing:
	‚Ä¢	Natural voice conversation
	‚Ä¢	Web search
	‚Ä¢	Barge-in
	‚Ä¢	Real-time context update
	‚Ä¢	Metrics dashboard

(Link provided in submission email)

‚∏ª

 Final Notes

This project is built as a realistic product system, not a toy demo. The emphasis is on correctness, resilience, and clarity of engineering decisions.

Thank you for reviewing!